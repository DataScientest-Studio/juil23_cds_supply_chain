{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import locale\n",
    "import dateparser\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/avis/df_cleaned.csv\",index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remettre en datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retirer les minutes / secondes / décalage\n",
    "df[\"date_avis\"] = df[\"date_avis\"].str.split(\" \").str[0]\n",
    "df[\"date_experience\"] = df[\"date_experience\"].str.split(\" \").str[0]\n",
    "\n",
    "### Transformation en datetime\n",
    "df[\"date_avis\"] = pd.to_datetime(df[\"date_avis\"], format='%Y-%m-%d')\n",
    "df[\"date_experience\"] = pd.to_datetime(df[\"date_experience\"], format='%Y-%m-%d')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter les colonnes jour / mois / heure / année et les catégoriser + rajouter longueur des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ajout des colonnes jour / mois / heure / année pour avis\n",
    "df[\"jour_avis\"] = df[\"date_avis\"].dt.day\n",
    "df[\"mois_avis\"] = df[\"date_avis\"].dt.month\n",
    "df[\"heure_avis\"] = df[\"date_avis\"].dt.hour\n",
    "df[\"annee_avis\"] = df[\"date_avis\"].dt.year\n",
    "\n",
    "df[\"jour_experience\"] = df[\"date_experience\"].dt.day\n",
    "df[\"mois_experience\"] = df[\"date_experience\"].dt.month\n",
    "df[\"heure_experience\"] = df[\"date_experience\"].dt.hour\n",
    "df[\"annee_experience\"] = df[\"date_experience\"].dt.year\n",
    "\n",
    "### Catégoriser en jour et nuit\n",
    "df[\"periode_avis\"] = df[\"heure_avis\"].apply(lambda x: \"jour\" if 6 <= x <= 22 else \"nuit\")\n",
    "df[\"longueur_texte\"] = df[\"text_total\"].apply(len)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "df['text_total'] = df['text_total'].fillna('').astype(str)\n",
    "df['text_total'] = [message.lower() for message in df['text_total']]\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "def stop_stop(review):\n",
    "    filtre = []\n",
    "    texte = nlp(review)\n",
    "    liste_texte = list(texte)\n",
    "    for word in liste_texte:\n",
    "        if str(word) not in stop_words:\n",
    "            filtre.append(str(word))\n",
    "    return(\" \".join(filtre))\n",
    "\n",
    "liste_filtrée =[]\n",
    "round_count = 1\n",
    "for review in df.text_total:\n",
    "    if round_count % 100 == 0:\n",
    "        print(f\"{round_count} reviews passed of {len(df)} - {round(round_count/len(df)*100,2)}% done\")\n",
    "    filt_review = stop_stop(review)\n",
    "    liste_filtrée.append(filt_review)\n",
    "    round_count = round_count + 1\n",
    "df[\"text_stop\"] = liste_filtrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting lemmas\n",
    "batch_size = 1000\n",
    "liste_lemma = []\n",
    "i = 0\n",
    "for doc in nlp.pipe(df.text_stop, batch_size=batch_size):\n",
    "    i +=1\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    liste_lemma.append(\" \".join(lemmas))\n",
    "    print(i ,\"reviews passed\")\n",
    "df['text_lemma'] = liste_lemma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the consolidated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/avis/df_clean_noYC_lemma.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
