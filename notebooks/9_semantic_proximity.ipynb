{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics import make_scorer, classification_report, make_scorer, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "df = pd.read_excel('../data/avis/general_df_clean_sent_15k_manual_code.xlsx')\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_frais= [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_frais'])]\n",
    "theme_efficacite = [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_efficacité'])]\n",
    "theme_discrimination = [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_discrimination'])]\n",
    "theme_protexction = [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_protection'])]\n",
    "theme_cloture = [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_clôture'])]\n",
    "theme_prets = [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_prêts'])]\n",
    "theme_virements = [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_virements'])]\n",
    "theme_communication = [row['text_avis'] for index, row in df.iterrows() if pd.notna(row['var_communication'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuggets_efficacité = pd.read_excel(\"../data/avis/general_df_clean_sent_15k_manual_code.xlsx\",\n",
    "                                   sheet_name = \"curation_efficacité\", index_col= 0)\n",
    "nuggets_efficacité.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tout\n",
    "list_exemples_tout = df.text_avis\n",
    "\n",
    "# efficace \n",
    "list_exemples_efficace = nuggets_efficacité.curated_efficace\n",
    "list_exemples_efficace = list_exemples_efficace.dropna()\n",
    "bag_efficace = \" \".join([message for message in list_exemples_efficace])\n",
    "\n",
    "# innefficace\n",
    "list_exemples_innefficace = nuggets_efficacité.curated_inneficace\n",
    "list_exemples_innefficace = list_exemples_innefficace.dropna()\n",
    "bag_innefficace = \" \".join([message for message in list_exemples_innefficace])\n",
    "bag_innefficace_nlp =nlp(bag_innefficace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonctions et liste de seuils de similarité à tester\n",
    "\n",
    "import numpy as np\n",
    "test_thresh = np.arange(0.59,1.01, 0.01)\n",
    "#thresh_bench = pd.DataFrame(columns=[\"thresh\", \"f1-score\"])\n",
    "f1_scores = []\n",
    "\n",
    "def allocate_lab(sim_score, y_pred, sim_thresh):\n",
    "    if max(sim_score) > sim_thresh:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "def similarity_report(y,y_pred):\n",
    "    report =classification_report(y, y_pred, output_dict=True)  \n",
    "    macro_f1 = round(report['macro avg']['f1-score'],2)\n",
    "    #print(report)\n",
    "    f1_scores.append(macro_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [message] to [bag of exemples] ->> f1 =0.7 avec thresh de 0.84\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in test_thresh:\n",
    "    y_pred = []\n",
    "    for message in list_exemples_tout:\n",
    "        sim_score = []\n",
    "        #print(\"message entier : \",message)\n",
    "        message = nlp(message)\n",
    "        sim_score.append(round(bag_innefficace_nlp.similarity(message),2))\n",
    "        #print(sim_score)\n",
    "        #print(\"-------------\")\n",
    "        allocate_lab(sim_score,y_pred,thresh)\n",
    "    similarity_report(y,y_pred)\n",
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [sentence of messages] vs [bag of exemples] --> score f1 : 0.73 avec thresh de 0.84\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in test_thresh:\n",
    "    y_pred = []\n",
    "    for message in list_exemples_tout:\n",
    "        sim_score= []\n",
    "        #print(\"message entier : \",message)\n",
    "        message = nlp(message)\n",
    "        sentences = message.sents\n",
    "        for sentence in sentences:\n",
    "            #print(\"phrase: \",sentence)\n",
    "            temp_sim_score = round(bag_innefficace_nlp.similarity(sentence),2)\n",
    "            #print(temp_sim_score)\n",
    "            sim_score.append(temp_sim_score)\n",
    "        #print(\"-------------\")\n",
    "        allocate_lab(sim_score,y_pred,thresh)\n",
    "    similarity_report(y,y_pred)\n",
    "\n",
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [sentence of messages] vs [étalon] --> f1 .57 avec seuil de 0.59\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in test_thresh:\n",
    "    y_pred = []\n",
    "    for message in list_exemples_tout:\n",
    "        sim_score= []\n",
    "        #print(\"message entier : \",message)\n",
    "        message = nlp(message)\n",
    "        sentences = message.sents\n",
    "        for sentence in sentences:\n",
    "            #print(\"phrase: \",sentence)\n",
    "            temp_sim_score = round(étalon.similarity(sentence),2)\n",
    "            #print(temp_sim_score)\n",
    "            sim_score.append(temp_sim_score)\n",
    "        #print(\"-------------\")\n",
    "        allocate_lab(sim_score,y_pred,thresh)\n",
    "    similarity_report(y,y_pred)\n",
    "\n",
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [sentence of messages] vs [étalon] --> f1 .54 avec seuil de 0.59 \n",
    "f1_scores = []\n",
    "\n",
    "for thresh in test_thresh:\n",
    "    y_pred = []\n",
    "    for message in list_exemples_tout:\n",
    "        sim_score = []\n",
    "        #print(\"message entier : \",message)\n",
    "        message = nlp(message)\n",
    "        sim_score.append(round(étalon.similarity(message),2))\n",
    "        #print(sim_score)\n",
    "        #print(\"-------------\")\n",
    "        allocate_lab(sim_score,y_pred,thresh)\n",
    "    similarity_report(y,y_pred)\n",
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [sentences of messages] vs [sentences of bag of exemples] -> f1-score of .84 avec seuil de 0.91\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in test_thresh:\n",
    "    y_pred = []\n",
    "    for message in list_exemples_tout:\n",
    "        sim_score= []\n",
    "        #print(\"message entier : \",message)\n",
    "        message = nlp(message)\n",
    "        sentences = message.sents\n",
    "        sentences_exemples = bag_innefficace_nlp.sents\n",
    "        for sentence in sentences:\n",
    "            for sentence_exemple in sentences_exemples:\n",
    "                #print(\"phrase: \",sentence)\n",
    "                temp_sim_score = round(sentence_exemple.similarity(sentence),2)\n",
    "                #print(temp_sim_score)\n",
    "                sim_score.append(temp_sim_score)\n",
    "        #print(\"-------------\")\n",
    "        allocate_lab(sim_score,y_pred,thresh)\n",
    "    similarity_report(y,y_pred)\n",
    "\n",
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_excel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version sentence sans stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = \"def\"\n",
    "exemples = \"str concaténé des exemples\"\n",
    "defex = definition + \" \" + exemples\n",
    "\n",
    "# boucle pour évaluer pour chaque theme les messages, puis les phrases dans chaque message\n",
    "\n",
    "# boucle thèmes\n",
    "for thème in df.themes:\n",
    "\tcol_theme = \"Q\" + thème\n",
    "\t# boucle pour les messages\n",
    "\tfor i, message in enumerate(train_df.text_total):\n",
    "\t\tmessage = nlp(message)\n",
    "\t\tmessage_sentence = sentence(message)\n",
    "\t\tsim_scores = []\n",
    "\t\tfor sentence in message_sentence:\n",
    "\t\t\tsim = sentence.similarity(defex)\n",
    "\t\t\tsim_scores.append(sim)\n",
    "\t\tif sim_score.max() > 0.8:\n",
    "\t\t\ttrain_df.iloc[i,col_theme] = 1\n",
    "\t\t\texerpt_col = col_theme + \"exerpt\"\n",
    "\t\t\ttrain_df.iloc[i,exerpt_col] = sentence[sim_scores.index(max(sim_scores))]\n",
    "\t\telse:\n",
    "\t\t\ttrain_df[col_theme] = 0\n",
    "\t\t\texerpt_col = \"\"\n",
    " \n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.72 using similarity thresh of 0.8300000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#  TEST [sentences of messages] vs [sentences of bag of exemples] -> 0.72 using similarity thresh of 0.83\n",
    "test = pd.read_excel(\"../data/avis/general_df_clean_sent_15k_manual_code.xlsx\",\n",
    "                                   sheet_name = \"test\", index_col= 0)\n",
    "test_text = test.text_avis\n",
    "y = test.var_efficacité_bad\n",
    "test_messages = test.text_avis.dropna()\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in test_thresh:\n",
    "    y_pred = []\n",
    "    for message in test_messages:\n",
    "        sim_score= []\n",
    "        #print(\"message entier : \",message)\n",
    "        message = nlp(message)\n",
    "        sentences = message.sents\n",
    "        sentences_exemples = bag_innefficace_nlp.sents\n",
    "        for sentence in sentences:\n",
    "            for sentence_exemple in sentences_exemples:\n",
    "                #print(\"phrase: \",sentence)\n",
    "                temp_sim_score = round(sentence_exemple.similarity(sentence),2)\n",
    "                #print(temp_sim_score)\n",
    "                sim_score.append(temp_sim_score)\n",
    "        #print(\"-------------\")\n",
    "        allocate_lab(sim_score,y_pred,thresh)\n",
    "    similarity_report(y,y_pred)\n",
    "\n",
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/gxswx1js22x0hszgmzpd29qcg1657r/T/ipykernel_94625/1558276724.py:19: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  temp_sim_score = round(sentence_exemple.similarity(sentence),2)\n"
     ]
    }
   ],
   "source": [
    "# essai sur l'ensemble de la base d'entrainement pour la variable inefficacité\n",
    "\n",
    "df = pd.read_csv(\"../data/avis/train_noYC_lemma_sent_equil.csv\", index_col= 0)\n",
    "df_messages = df.text_total\n",
    "\n",
    "f1_scores = []\n",
    "thresh = 0.83 # as per our grid search results\n",
    "\n",
    "y_pred = []\n",
    "for message in df_messages:\n",
    "    sim_score= []\n",
    "    #print(\"message entier : \",message)\n",
    "    message = nlp(message)\n",
    "    sentences = message.sents\n",
    "    sentences_exemples = bag_innefficace_nlp.sents\n",
    "    for sentence in sentences:\n",
    "        for sentence_exemple in sentences_exemples:\n",
    "            #print(\"phrase: \",sentence)\n",
    "            temp_sim_score = round(sentence_exemple.similarity(sentence),2)\n",
    "            #print(temp_sim_score)\n",
    "            sim_score.append(temp_sim_score)\n",
    "    #print(\"-------------\")\n",
    "    allocate_lab(sim_score,y_pred,thresh)\n",
    "\n",
    "df['var_bad:inefficace'] = y_pred\n",
    "\n",
    "best_score = max(f1_scores)\n",
    "best_thresh = test_thresh[f1_scores.index(best_score)]\n",
    "print(f\"best score: {best_score} using similarity thresh of {best_thresh}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
