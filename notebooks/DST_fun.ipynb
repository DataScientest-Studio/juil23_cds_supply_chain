{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de vectorisation et de reporting\n",
    "def model_report():\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    # measuring time taken to train the model\n",
    "    t1 = time.time()\n",
    "    delais = round((t1-t0)/60,2)\n",
    "    # test score\n",
    "    try:\n",
    "        score = round(model.score(X_test, y_test),2)\n",
    "    except:\n",
    "        score =  \"na\"\n",
    "    print(\"train score: \", score)\n",
    "\n",
    "    # predictiong on test set, accomodating to dm matrix in except (test contains X and y)\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "    except:\n",
    "        y_pred = model.predict(test)\n",
    "    \n",
    "    # saving results in the benchmark file\n",
    "    try:\n",
    "        extra_features_rep = X_train.columns.values\n",
    "    except:\n",
    "        extra_features_rep = extra_features\n",
    "       \n",
    "    if \"Grid\" in type(model).__name__:\n",
    "        bool_grid = \"yes\"\n",
    "    else:\n",
    "        bool_grid = \"no\"\n",
    "  \n",
    "    if bool_grid == \"yes\":\n",
    "        used_params = model.best_params_\n",
    "    else:\n",
    "        used_params = \"baseline hyperparameters\"\n",
    "\n",
    "    model_name = type(model).__name__\n",
    "    report =classification_report(y_test, y_pred, output_dict=True)\n",
    "    macro_precision =  round(report['macro avg']['precision'],2) \n",
    "    macro_recall = round(report['macro avg']['recall'],2)    \n",
    "    macro_f1 = round(report['macro avg']['f1-score'],2)  \n",
    "    tempdf = pd.DataFrame({\"model\":[model_type],\n",
    "                            \"grid search\": [bool_grid],\n",
    "                            \"used/best params\":[used_params],\n",
    "                            \"features\": [extra_features_rep],\n",
    "                            \"score\":[score],\n",
    "                            \"precision\": [macro_precision],\n",
    "                            \"recall\": [macro_recall],\n",
    "                            \"f1\":[macro_f1],\n",
    "                            \"time_taken_mns\":[delais],\n",
    "                            \"run_date\": [time.strftime('%Y-%m-%d', time.localtime())]\n",
    "                        })\n",
    "    # reports: classification report and crosstab heatmap \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Generate and normalize the confusion matrix\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    # Create a heatmap for the confusion matrix\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(conf_mat_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.title(f'Normalized Confusion Matrix for {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # load and append results to the benchmark, save\n",
    "    bench = pd.read_csv('../reports/model_benchmark.csv', index_col=0)\n",
    "    bench = pd.concat([bench, tempdf])\n",
    "    bench.to_csv('../reports/model_benchmark.csv')\n",
    "\n",
    "def review_vector(df,raw_extra_features):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    extra_features = raw_extra_features\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # 1000 pour garder l'essentiel, plus?\n",
    "    vec_text = tfidf_vectorizer.fit_transform(df['text_lemma'])\n",
    "    #print(vec_text[0:5])\n",
    "    # Ajouter les variables en format dense, comme le texte vectoris√©\n",
    "    df_tf = hstack([vec_text, csr_matrix(df[extra_features])])\n",
    "    return df_tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
