{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etoiles</th>\n",
       "      <th>n_avis</th>\n",
       "      <th>date_avis</th>\n",
       "      <th>text_total</th>\n",
       "      <th>codes agrégés</th>\n",
       "      <th>c_good_value</th>\n",
       "      <th>c_bad_value</th>\n",
       "      <th>c_good_efficacy</th>\n",
       "      <th>c_bad_efficacy</th>\n",
       "      <th>c_good_comm</th>\n",
       "      <th>c_bad_comm</th>\n",
       "      <th>text_sent_noPunct</th>\n",
       "      <th>text_clean_neg</th>\n",
       "      <th>text_clean_post</th>\n",
       "      <th>text_clean_pos</th>\n",
       "      <th>text_clean_pos_whole</th>\n",
       "      <th>text_clean_neg_whole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-07-26 10:10:19</td>\n",
       "      <td>pour l'instant toujours en attente. pour l'ins...</td>\n",
       "      <td>bad:inefficace</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[pour l' instant toujours en attente, pour l' ...</td>\n",
       "      <td>[pour l' instant toujours en attente, pour l' ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>pour l' instant toujours en attente pour l' in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-04-11 14:14:30</td>\n",
       "      <td>intérêts trop élevés.</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[intérêts trop élevés]</td>\n",
       "      <td>[intérêts trop élevés]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>intérêts trop élevés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-06-07 16:57:39</td>\n",
       "      <td>c'était les meilleurs, mais c'était avant. deu...</td>\n",
       "      <td>bad:communication;bad:inefficace</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[c' était les meilleurs mais c' était avant, d...</td>\n",
       "      <td>[c' était les meilleurs mais c' était avant, d...</td>\n",
       "      <td>[pour moi fortunéo était la meilleure banque m...</td>\n",
       "      <td>[pour moi fortunéo était la meilleure banque m...</td>\n",
       "      <td>pour moi fortunéo était la meilleure banque ma...</td>\n",
       "      <td>c' était les meilleurs mais c' était avant deu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-09-08 13:30:04</td>\n",
       "      <td>vous n'aidez pas les gens. vous n'aidez pas le...</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vous n' aidez pas les gens, vous n' aidez pas...</td>\n",
       "      <td>[vous n' aidez pas les gens, vous n' aidez pas...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>vous n' aidez pas les gens vous n' aidez pas l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-02-10 20:32:37</td>\n",
       "      <td>de voyage à l'étranger mon conseillé de…. de v...</td>\n",
       "      <td>bad:clôture_abusive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[de voyage à l' étranger, mon conseillé de, de...</td>\n",
       "      <td>[de voyage à l' étranger mon conseillé de la b...</td>\n",
       "      <td>[de voyage à l' étranger, mon conseillé de]</td>\n",
       "      <td>[de voyage à l' étranger, mon conseillé de]</td>\n",
       "      <td>de voyage à l' étranger mon conseillé de</td>\n",
       "      <td>de voyage à l' étranger mon conseillé de la ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     etoiles  n_avis            date_avis  \\\n",
       "NaN        3     3.0  2022-07-26 10:10:19   \n",
       "NaN        3     1.0  2022-04-11 14:14:30   \n",
       "NaN        2     3.0  2023-06-07 16:57:39   \n",
       "NaN        1     1.0  2023-09-08 13:30:04   \n",
       "NaN        1     2.0  2023-02-10 20:32:37   \n",
       "\n",
       "                                            text_total  \\\n",
       "NaN  pour l'instant toujours en attente. pour l'ins...   \n",
       "NaN                              intérêts trop élevés.   \n",
       "NaN  c'était les meilleurs, mais c'était avant. deu...   \n",
       "NaN  vous n'aidez pas les gens. vous n'aidez pas le...   \n",
       "NaN  de voyage à l'étranger mon conseillé de…. de v...   \n",
       "\n",
       "                        codes agrégés  c_good_value  c_bad_value  \\\n",
       "NaN                    bad:inefficace             0            0   \n",
       "NaN                              none             0            0   \n",
       "NaN  bad:communication;bad:inefficace             0            0   \n",
       "NaN                              none             0            0   \n",
       "NaN               bad:clôture_abusive             0            0   \n",
       "\n",
       "     c_good_efficacy  c_bad_efficacy  c_good_comm  c_bad_comm  \\\n",
       "NaN                0               1            0           0   \n",
       "NaN                0               0            0           0   \n",
       "NaN                0               1            0           1   \n",
       "NaN                0               0            0           0   \n",
       "NaN                0               1            0           0   \n",
       "\n",
       "                                     text_sent_noPunct  \\\n",
       "NaN  [pour l' instant toujours en attente, pour l' ...   \n",
       "NaN                             [intérêts trop élevés]   \n",
       "NaN  [c' était les meilleurs mais c' était avant, d...   \n",
       "NaN  [vous n' aidez pas les gens, vous n' aidez pas...   \n",
       "NaN  [de voyage à l' étranger, mon conseillé de, de...   \n",
       "\n",
       "                                        text_clean_neg  \\\n",
       "NaN  [pour l' instant toujours en attente, pour l' ...   \n",
       "NaN                             [intérêts trop élevés]   \n",
       "NaN  [c' était les meilleurs mais c' était avant, d...   \n",
       "NaN  [vous n' aidez pas les gens, vous n' aidez pas...   \n",
       "NaN  [de voyage à l' étranger mon conseillé de la b...   \n",
       "\n",
       "                                       text_clean_post  \\\n",
       "NaN                                                 []   \n",
       "NaN                                                 []   \n",
       "NaN  [pour moi fortunéo était la meilleure banque m...   \n",
       "NaN                                                 []   \n",
       "NaN        [de voyage à l' étranger, mon conseillé de]   \n",
       "\n",
       "                                        text_clean_pos  \\\n",
       "NaN                                                 []   \n",
       "NaN                                                 []   \n",
       "NaN  [pour moi fortunéo était la meilleure banque m...   \n",
       "NaN                                                 []   \n",
       "NaN        [de voyage à l' étranger, mon conseillé de]   \n",
       "\n",
       "                                  text_clean_pos_whole  \\\n",
       "NaN                                                      \n",
       "NaN                                                      \n",
       "NaN  pour moi fortunéo était la meilleure banque ma...   \n",
       "NaN                                                      \n",
       "NaN           de voyage à l' étranger mon conseillé de   \n",
       "\n",
       "                                  text_clean_neg_whole  \n",
       "NaN  pour l' instant toujours en attente pour l' in...  \n",
       "NaN                               intérêts trop élevés  \n",
       "NaN  c' était les meilleurs mais c' était avant deu...  \n",
       "NaN  vous n' aidez pas les gens vous n' aidez pas l...  \n",
       "NaN  de voyage à l' étranger mon conseillé de la ba...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer une version du texte listé par phrases de plus de un mot et sans ponctuation\n",
    "train['text_sent_noPunct'] = \"\"\n",
    "def create_review_sent_noPunct(df)\n",
    "for i, review in enumerate(df.text_total):\n",
    "    # Reconstruct sentences without punctuation\n",
    "    review = nlp(review)\n",
    "    sentences_without_punctuation = []\n",
    "    for sent in review.sents:\n",
    "        # Filter out tokens that are punctuation\n",
    "        tokens_without_punctuation = [token.text for token in sent if not token.is_punct]\n",
    "        if len(tokens_without_punctuation) >1:   \n",
    "            # Join the tokens to form a new sentence\n",
    "            new_sentence = \" \".join(tokens_without_punctuation)\n",
    "            sentences_without_punctuation.append(new_sentence)\n",
    "    df['text_sent_noPunct'].iloc[i] = sentences_without_punctuation\n",
    "\n",
    "# créer deux colones: une pour les phrases positives, l'autre pour les négatives\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import pipeline,AutoTokenizer, TFCamembertForSequenceClassification\n",
    "import sentencepiece\n",
    "\n",
    "# importing spacy model\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "\n",
    "# importing  tokenizer and Camembert classifier\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\")\n",
    "model = TFCamembertForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "classifier=  pipeline(\"text-classification\", model = model, tokenizer= tokenizer)\n",
    "#tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512,'return_tensors':'pt'}\n",
    "\n",
    "\n",
    "\n",
    "def creating_cols_pos_neg_sents(df):\n",
    "    df['text_clean_neg'] = \"\"\n",
    "    df['text_clean_pos'] = \"\"\n",
    "    for i, sentence_list in enumerate(df['text_sent_noPunct']):\n",
    "        pos_sent, neg_sent = [],[]\n",
    "        print(\"new message\")\n",
    "        for sentence in sentence_list:\n",
    "            sentence = sentence[:512] # nombre de mots max pour Camembert\n",
    "            try:\n",
    "                temp = classifier(sentence)[0]['label']\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement du texte {sentence}. Erreur: {e}\")\n",
    "            if temp == \"POSITIVE\":\n",
    "                pos_sent.append(sentence)\n",
    "            elif temp == \"NEGATIVE\":\n",
    "                neg_sent.append(sentence)\n",
    "        df['text_clean_neg'].iloc[i] = neg_sent\n",
    "        df['text_clean_pos'].iloc[i] = pos_sent\n",
    "    \n",
    "    df[\"text_clean_pos_whole\"] = \"\"\n",
    "    for i,sentence_list in enumerate(df.text_clean_pos):\n",
    "        df[\"text_clean_pos_whole\"].iloc[i] = \" \".join([sent for sent in sentence_list])\n",
    "\n",
    "    df[\"text_clean_neg_whole\"] = \"\"\n",
    "    for i,sentence_list in enumerate(df.text_clean_neg):\n",
    "        df[\"text_clean_neg_whole\"].iloc[i] = \" \".join([sent for sent in sentence_list])\n",
    "\n",
    "\n",
    "creating_cols_pos_neg_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_review_vs_whole_ref_bag(bag_of_ref,reviews, y,code_of_interest,thresh, train_test, word_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_sentiment(sentiment,code_of_interest, sentence_whole):\n",
    "    if sentiment == True:\n",
    "        if \"good\" in code_of_interest:\n",
    "            if \"sentence\" in sentence_whole:\n",
    "                reviews =  \"text_clean_sentences_pos\"\n",
    "            elif \"whole\" in sentence_whole:\n",
    "                reviews = \"text_clean_pos_whole\"    \n",
    "        elif \"bad\" in code_of_interest:\n",
    "            if \"sentence\" in sentence_whole:\n",
    "                reviews =  \"text_clean_sentences_neg \"\n",
    "            elif \"whole\" in sentence_whole:\n",
    "                reviews = \"text_clean_neg_whole\"\n",
    "    elif sentiment == False:\n",
    "        if \"whole\" in sentence_whole:\n",
    "            reviews = \"text_sent_noPunct_whole\"\n",
    "        elif \"sentence\" in sentence_whole:\n",
    "            reviews = \"text_sent_noPunct_sentences\"\n",
    "    else :\n",
    "        print(\"error: no sentiment / granularity found\")\n",
    "    return reviews\n",
    "            \n",
    "\n",
    "def core_full_review_vs_whole_ref_bag(bag_of_ref,df, thresh,code_of_interest, sentiment):\n",
    "    y_pred = []\n",
    "    reviews = decision_sentiment(sentiment, code_of_interest, \"whole\")\n",
    "    df[reviews]\n",
    "    for message in reviews:\n",
    "        print (message) \n",
    "        sim_score = []\n",
    "        message = nlp(message)\n",
    "        sim_score.append(round(bag_of_ref.similarity(message),2))\n",
    "        allocate_lab(sim_score, y_pred, thresh)\n",
    "    return y_pred\n",
    "\n",
    "def core_review_sentence_vs_whole_ref_bag(bag_of_ref,reviews, thresh):\n",
    "    y_pred = []\n",
    "    for message in reviews:\n",
    "        sim_score = []\n",
    "        message = nlp(message)\n",
    "        sentences = message.sents\n",
    "        for sentence in sentences:\n",
    "            if sentence.has_vector:\n",
    "                try:\n",
    "                    temp_sim_score = round(bag_of_ref.similarity(sentence),2)\n",
    "\n",
    "                    sim_score.append(temp_sim_score)\n",
    "                except:\n",
    "                    print(\"issue with vector\")\n",
    "                    sim_score.append(0)\n",
    "        allocate_lab(sim_score, y_pred, thresh)\n",
    "    return y_pred\n",
    "\n",
    "def core_review_sentence_vs_reference_sentence(bag_of_ref,reviews, thresh):\n",
    "    y_pred,trigger_ref_sent = [],[]\n",
    "    for message in reviews:\n",
    "        sim_score= []\n",
    "        message = nlp(message)\n",
    "        review_sentences = message.sents\n",
    "        sentences_exemples = bag_of_ref.sents\n",
    "        for review_sentence in review_sentences:\n",
    "            if review_sentence.has_vector:\n",
    "                for sentence_exemple in sentences_exemples:\n",
    "                    if sentence_exemple.has_vector:\n",
    "                        try:\n",
    "                            temp_sim_score = round(review_sentence.similarity(sentence_exemple),2)\n",
    "                            sim_score.append(temp_sim_score)\n",
    "                        except:\n",
    "                            print(\"issue with vector\")\n",
    "                            sim_score.append(0.0)\n",
    "        print('sim score', sim_score)\n",
    "        best_sim_score =  max(sim_score)\n",
    "        list_trigger_ref_sents = [sent for sent in sentences_exemples]\n",
    "        trigger_ref_sent.append(list_trigger_ref_sents[sim_score.index(best_sim_score)])\n",
    "        allocate_lab(sim_score, y_pred, thresh)\n",
    "    return y_pred, trigger_ref_sent\n",
    "\n",
    "### toujours le meme f1 score??? Vaut pour les deux étalons est ce que sim score est toujours sous le seuil?\n",
    "def core_review_sentence_vs_ref_étalon(etalon,reviews, thresh):\n",
    "    y_pred = []\n",
    "    etalon = nlp(etalon)\n",
    "    for message in reviews:\n",
    "        sim_score= []\n",
    "        message = nlp(message)\n",
    "        sentences = message.sents\n",
    "        for sentence in sentences:\n",
    "            if sentence.has_vector:\n",
    "                try:\n",
    "                    temp_sim_score = round(etalon.similarity(sentence),2)\n",
    "                    sim_score.append(temp_sim_score)\n",
    "                    #print(\"sim score review sentence:\", temp_sim_score)\n",
    "                except:\n",
    "                    print(\"issue with vector\")\n",
    "                    sim_score.append(0)\n",
    "        allocate_lab(sim_score, y_pred, thresh)\n",
    "    return y_pred\n",
    "        \n",
    "def core_full_review_vs_étalon(etalon,reviews, thresh):\n",
    "    y_pred = []\n",
    "    etalon = nlp(etalon)\n",
    "    for message in reviews:\n",
    "        sim_score = []\n",
    "        message = nlp(message)\n",
    "        sim_score.append(round(etalon.similarity(message),2))\n",
    "        print(\"sim score review sentence:\", sim_score[-1])\n",
    "        allocate_lab(sim_score, y_pred, thresh)\n",
    "    return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
